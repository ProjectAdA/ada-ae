package de.ada.restapi;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

public class MetadataConstants {
	
	public static final List<Map<String, Object>> AUTOMATED_ANALYSIS_TYPES = new ArrayList<Map<String,Object>>();
	public static final Map<String, Object> AUTOMATED_ANALYSIS_LEVEL  = new HashMap<String, Object>();
	public static final Set<String> EXTRACTORS = new HashSet<String>();
	
	static {
		AUTOMATED_ANALYSIS_LEVEL.put("elementUri","https://github.com/ProjectAdA/ada-va");
		AUTOMATED_ANALYSIS_LEVEL.put("id","AnnotationLevel/AutomatedAnalysis");
		AUTOMATED_ANALYSIS_LEVEL.put("elementName","Automated Video Analysis");
		AUTOMATED_ANALYSIS_LEVEL.put("elementDescription","Annotations generated by AdA audio-visual analysis tools.");
		AUTOMATED_ANALYSIS_LEVEL.put("sequentialNumber",10);
		AUTOMATED_ANALYSIS_LEVEL.put("elementFullName",null);
		AUTOMATED_ANALYSIS_LEVEL.put("elementColor",null);
		AUTOMATED_ANALYSIS_LEVEL.put("elementNumericValue",null);
		AUTOMATED_ANALYSIS_LEVEL.put("subElements",AUTOMATED_ANALYSIS_TYPES);
		
		Map<String,Object> sd = new HashMap<String, Object>();
		sd.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/shotdetection");
		sd.put("id","AnnotationType/shotdetection");
		sd.put("elementName","Shot Boundary Detector");
		sd.put("elementFullName","Auto | shotdetection");
		sd.put("elementDescription","Extract shot boundaries in a video media file (uses https://github.com/johmathe/Shotdetect).");
		sd.put("sequentialNumber",100);
		sd.put("elementColor","#cdad00");
		sd.put("elementNumericValue",null);
		sd.put("maxNumericValue",null);
		sd.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(sd);
		EXTRACTORS.add("shotdetection");

		Map<String,Object> of = new HashMap<String, Object>();
		of.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/motion_dynamics");
		of.put("id","AnnotationType/motion_dynamics");
		of.put("elementName","Motion Dynamics Extractor");
		of.put("elementFullName","Auto | motion_dynamics");
		of.put("elementDescription","Extract the motion vector magnitudes using optical flow.");
		of.put("sequentialNumber",101);
		of.put("elementColor","#cdad00");
		of.put("elementNumericValue",null);
		of.put("maxNumericValue",null);
		of.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(of);
		EXTRACTORS.add("motion_dynamics");

		Map<String,Object> asr = new HashMap<String, Object>();
		asr.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/deepspeech");
		asr.put("id","AnnotationType/deepspeech");
		asr.put("elementName","DeepSpeech ASR");
		asr.put("elementFullName","Auto | deepspeech");
		asr.put("elementDescription","Transcribes the spoken audio of a video using Mozilla DeepSpeech (uses https://github.com/mozilla/DeepSpeech/). Currently, only English language is supported.");
		asr.put("sequentialNumber",102);
		asr.put("elementColor","#cdad00");
		asr.put("elementNumericValue",null);
		asr.put("maxNumericValue",null);
		asr.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(asr);
		EXTRACTORS.add("deepspeech");

		Map<String,Object> ar = new HashMap<String, Object>();
		ar.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/aspect_ratio");
		ar.put("id","AnnotationType/aspect_ratio");
		ar.put("elementName","Aspect Ratio Extractor");
		ar.put("elementFullName","Auto | aspect_ratio");
		ar.put("elementDescription","Extract the aspect ratio in a video media file on a shot level.");
		ar.put("sequentialNumber",103);
		ar.put("elementColor","#cdad00");
		ar.put("elementNumericValue",null);
		ar.put("maxNumericValue",null);
		ar.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(ar);
		EXTRACTORS.add("aspect_ratio");

		Map<String,Object> im = new HashMap<String, Object>();
		im.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/im2txt");
		im.put("id","AnnotationType/im2txt");
		im.put("elementName","im2txt Frame Captioning");
		im.put("elementFullName","Auto | im2txt");
		im.put("elementDescription","Extracts textual frame descriptions using the Show and Tell deep neural network (uses https://github.com/tensorflow/models/tree/archive/research/im2txt).");
		im.put("sequentialNumber",104);
		im.put("elementColor","#cdad00");
		im.put("elementNumericValue",null);
		im.put("maxNumericValue",null);
		im.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(im);
		EXTRACTORS.add("im2txt");

		Map<String,Object> dc = new HashMap<String, Object>();
		dc.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/densecap");
		dc.put("id","AnnotationType/densecap");
		dc.put("elementName","DenseCap Frame Captioning");
		dc.put("elementFullName","Auto | densecap");
		dc.put("elementDescription","Extracts dense captionings from video frames (uses https://github.com/jcjohnson/densecap).");
		dc.put("sequentialNumber",105);
		dc.put("elementColor","#cdad00");
		dc.put("elementNumericValue",null);
		dc.put("maxNumericValue",null);
		dc.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(dc);
		EXTRACTORS.add("densecap");

		Map<String,Object> se = new HashMap<String, Object>();
		se.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/soundenvelope");
		se.put("id","AnnotationType/soundenvelope");
		se.put("elementName","Sound Envelope Extractor");
		se.put("elementFullName","Auto | soundenvelope");
		se.put("elementDescription","Extracts the sound envelope for the audio stream of a video media file (uses Advene https://github.com/oaubert/advene/blob/master/lib/advene/plugins/soundenveloppe.py).");
		se.put("sequentialNumber",106);
		se.put("elementColor","#cdad00");
		se.put("elementNumericValue",null);
		se.put("maxNumericValue",null);
		se.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(se);
		EXTRACTORS.add("soundenvelope");

		Map<String,Object> ynm = new HashMap<String, Object>();
		ynm.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/yamnet");
		ynm.put("id","AnnotationType/yamnet_music");
		ynm.put("elementName","YAMNnet Music Extractor");
		ynm.put("elementFullName","Auto | yamnet_music");
		ynm.put("elementDescription","Extracts audio event class 'music' for the audio stream of a video media file (uses https://github.com/tensorflow/models/tree/archive/research/audioset/yamnet).");
		ynm.put("sequentialNumber",107);
		ynm.put("elementColor","#cdad00");
		ynm.put("elementNumericValue",null);
		ynm.put("maxNumericValue",null);
		ynm.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(ynm);
		EXTRACTORS.add("yamnet_music");

		Map<String,Object> ynsp = new HashMap<String, Object>();
		ynsp.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/yamnet");
		ynsp.put("id","AnnotationType/yamnet_speech");
		ynsp.put("elementName","YAMNnet Speech Extractor");
		ynsp.put("elementFullName","Auto | yamnet_speech");
		ynsp.put("elementDescription","Extracts audio event class 'speech' for the audio stream of a video media file (uses https://github.com/tensorflow/models/tree/archive/research/audioset/yamnet).");
		ynsp.put("sequentialNumber",108);
		ynsp.put("elementColor","#cdad00");
		ynsp.put("elementNumericValue",null);
		ynsp.put("maxNumericValue",null);
		ynsp.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(ynsp);
		EXTRACTORS.add("yamnet_speech");

		Map<String,Object> ynsi = new HashMap<String, Object>();
		ynsi.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/yamnet");
		ynsi.put("id","AnnotationType/yamnet_silence");
		ynsi.put("elementName","YAMNnet Silence Extractor");
		ynsi.put("elementFullName","Auto | yamnet_silence");
		ynsi.put("elementDescription","Extracts audio event class 'silence' for the audio stream of a video media file (uses https://github.com/tensorflow/models/tree/archive/research/audioset/yamnet).");
		ynsi.put("sequentialNumber",109);
		ynsi.put("elementColor","#cdad00");
		ynsi.put("elementNumericValue",null);
		ynsi.put("maxNumericValue",null);
		ynsi.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(ynsi);
		EXTRACTORS.add("yamnet_silence");

		Map<String,Object> yno = new HashMap<String, Object>();
		yno.put("elementUri","https://github.com/ProjectAdA/ada-va/tree/main/ada-va/extractors/yamnet");
		yno.put("id","AnnotationType/yamnet_other");
		yno.put("elementName","YAMNnet Other Extractor");
		yno.put("elementFullName","Auto | yamnet_other");
		yno.put("elementDescription","Extracts other audio event classes for the audio stream of a video media file (uses https://github.com/tensorflow/models/tree/archive/research/audioset/yamnet).");
		yno.put("sequentialNumber",110);
		yno.put("elementColor","#cdad00");
		yno.put("elementNumericValue",null);
		yno.put("maxNumericValue",null);
		yno.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(yno);
		EXTRACTORS.add("yamnet_other");

		/*		Map<String,Object> nt = new HashMap<String, Object>();
		nt.put("elementUri","https://github.com/karpathy/neuraltalk2");
		nt.put("id","AnnotationType/neuraltalk2");
		nt.put("elementName","Neuraltalk2");
		nt.put("elementFullName","Auto | neuraltalk2");
		nt.put("elementDescription","Image Captioning with Neuraltalk2");
		nt.put("sequentialNumber",111);
		nt.put("elementColor","#cdad00");
		nt.put("elementNumericValue",null);
		nt.put("maxNumericValue",null);
		nt.put("subElements",null);
		AUTOMATED_ANALYSIS_TYPES.add(nt);
		EXTRACTORS.add("neuraltalk2");
*/


	}
}
